% Min: you need a paragraph at the beginning to describe what topics (subsections) you're going to talk wbout and why.
In order to devise such an algorithm, we need to predict how often any user may 
update a page. Some work has been done to try to predict how often page content 
is updated, with the aim of scheduling download times in order to keep a local 
database fresh.

% Min: this chapter needs A LOT OF WORK.  The work you review is piecemeal at best and does not represent a comprehensive survey of the work in the area.  I would cut your losses and try to make a coherent related work chapter instead of including one (mostly irrelevant) work from various tangential areas.

\section{Refresh policies for incremental crawlers}

We first discuss the \emph{timeliness} of our crawler to maintain the freshness 
of the local database, which refers to how new the extracted information is. Web 
crawlers can be used to crawl sites for user comments for 
later post-processing. Web crawlers which maintain the freshness of a database of 
crawled content are known as incremental crawlers. Two trade-offs these crawlers 
face cited by \outcite{Yang2009} are \emph{completeness} and \emph{timeliness}.  
\emph{Completeness} refers to the extent which the crawler fetches all the 
pages, without missing any pages. \emph{Timeliness} refers to the efficiency 
with which the crawler discovers and downloads newly-created content. We focus 
mainly on timeliness in this project, as we believe that timely updates of 
active threads are more important than complete archival of all threads in the 
forum site.

Many such works have used the Poisson distribution to model page updates.  
% Min: why?  Why is it a good fit for the model?
\outcite{Coffman1997} analysed the theoretical aspects of doing this, showing 
that if the page change process is governed by a Poisson process 
$\frac{\lambda^k e^{-\lambda \mu}}{k!}$, then accessing the page at intervals 
proportional to $\lambda$ is optimal.

Cho and Garcia-Molina trace the change history of 720,000 web pages collected 
over four months, and showed empirically that the Poisson process model closely 
matches the update processes found in web pages \cite{Cho1999}. They then 
proposed different revisiting or refresh policies 
\cite{Cho2003,Garcia-molina2003} that attempt to maintain the freshness of the 
database.

The Poisson distribution were also used in \outcite{Tan2007} and 
\outcite{Wolf2002}. %elaborate!!!!
% Min: agreed with the above comment.  Why are these works worth mentioning?  How do they inform your work?
However, the Poisson distribution is memoryless, and in experimental results due 
to \outcite{Brian2000}, the behaviour of site updates are not. Moreover, these 
studies were not performed specifically on online threads, where the behaviour 
of page updates differs from static pages.

\outcite{Yang2009}, attempted to resolve this by using the list structure of 
forum sites to infer a sitemap. With this, they reconstruct the full thread, and 
then use a linear-regression model to predict when the next update to the thread 
will arrive. 
%elaborate!!!
% Min: agreed.  Your description is about the technical, engineering details and not about the qctual work wrt to algorithms

Forums have a logical, hierarchical structure in their 
layout, which typically alerts the user to thread updates by putting threads 
with new replies at the top of the thread index. Yang's work exploits this 
as well as their linear model to achieve a predicton of when to retrieve the 
pages.
However this design pattern is not applied universally; 
% Min: sorry broke your sentence.  Can you fix it?  I've tried.
comments on blog sites 
or e-commerce sites about products do not conform to this pattern.  The lack of such 
information may result in a poorer estimate, or no estimate at all.

The above works all try to estimate the arrival of the next update (comment), but do not leverage an obvious source of information, which is the content of the posts themselves. Our perspective
is that the available thread content can be used to provide a better estimation for predicting page updates. 

Next, we look at some of the related work
pertaining to thread content.

\section{Thread content analysis}
While there is little existing work using content to predict page updates, we 
review existing work related to analysing thread-based pages.  We 
think such work will aid our efforts in content-based update prediction.

% Min: not clear what you mean by "links".  Need to elaborate.  Is this relevant?
\outcite{Wang2011} find links between forum posts using 
lexical chaining. They proposed a method to link posts using the tokens in the 
posts called $Chainer_{SV}$. While they analyse the contents of individual 
posts, the paper does not make any prediction with regards to newer posts. The 
methods used to produce a numeric similarities between posts may be used as a 
feature to describe a thread in its current state, 
% Min: this isn't clear.  You haven't even described what your current model is, so to say it's non-trivial doesn't seem justified.
but incorporating this into 
our model is non-trivial.

%Kleinberg used Hidden Markov Models to predict ``bursts" in message arrival 
%times \cite{Kleinberg2003}. In his running example, he used email messages, and 
%used time between messages to estimate the states that produced the sequence.  
%While the model may be able to predict what the state is for the next time 
%interval, it does so using the history of message arrival times, and does not 
%take into account the content within the messages themselves.


%One also cannot ignore the fact that social factors play a role when users 
%interact in an online discussion. Granovetter's threshold model for social 
%behaviour may also be useful in describing how the users behave as a whole.

% Min: this section is way too short.  You only examined one content related work and not even particularly relevant.  I expect trouble from your examiners.
% Min: This description of your work is out of place.  You can't talk about your work yet, not here.  You could have done an overview of your work in the introduction but it doesn't make sense to have it in the related work section.
With these related work in mind, we next propose our modelling of a thread as a 
Markov chain, and our approach to solving the problem.

\section{Evaluation metrics}
% Min: you need to preface this with a short paragraph to say why evaluation metrics need to be discussed.  I'm not sure this needs to be here; this whole section on related work for evaluation can go with the evaluation section itself.
 
\outcite{Yang2009} proposed a metric for our particular problem of thread update prediction. Known as the $T$-score, it gives the average 
time difference between when a post is made and when the post is retrieved by a crawler. The lower the $T$-score, the 
better the model. However, the metric does not penalize for visits which retrieve 
nothing new from the thread.  As such, a crawler that repeatedly crawls the site 
at a frequent rate would do very well.

Broadening our search for more relevant evaluation metrics that take such wasted bandwidth into account, we turn to related work in the evaluation of segmentation algorithms.  In \outcite{Georgescul2009}, the authors propose a new scheme for evaluating 
segmentation algorithms, $Pr_{error}$. 
% Min: you need to describe how this measure can be applied to your scenario.
This metric is the weighted sum of two 
probability counts $Pr_{fa}$ which is the probability that a false alarm 
segmentation is made, and $Pr_{miss}$ which is the probability that a 
segmentation is not made when there should be one. Unfortunately for our 
purposes, the metrics are calculated using the number of ground truths and 
segmentations given a window. As such, it does not account for the ``distance" 
between the ground truths and the segmentation. It also does not allow for the 
predictions to appear after the ground truths, all requirements needed for a 
metric to evaluate timeliness of a model.

\section{Predicting events in social media}
% Min: Aobo's work doesn't have much to do with your research.  I have no idea why you are including it here.  If relevant at all, it should go into your content analysis section.
There has been some work done recently in predicting events in social media, and 
in particular, tweets.  \outcite{Wang} dealt with predicting the retweetability 
of tweets using content. They applied two levels of classification, the first 
level categorising tweets into 6 different types: Opinion, Update, Interaction, 
Fact, Deals and Others. This was done using similar techniques as 
\outcite{Sriram2010} and \outcite{Naaman2010}. The Opinion and Update categories 
are then further categorised into another three and two sub-categories each. The 
authors performed this categorisation using labeled Latent Dirichlet Allocation 
%TODO:ITE.

The task in this work, was to predict which of three predefined classes a tweet 
will fall into: no retweets, a low number of retweets and a high number of retweets. 
% Min: still unsure of its usefulness.  Strongly suggest moving this to the section on (thread) content analysis.
Our task is slightly more challenging, 
since we are trying to minimise the time from which a post is made to when the 
page is revisited. However, the feature sets used in these works should prove 
useful in our task.  

% Min: I would drop this.  This is like adding 2000+ relevant works and choosing Cao2003 as a representative.  A very bad idea.
\section{Forecasting and Machine Learning}
Since the purpose of our envisioned model would be to create a crawler that can estimate the best times to revisit a page, a proper approach to modeling this as a time-series model would be through machine learning techniques.

An interesting approach to forecasting stock prices was presented in \outcite{Cao2003}. The technique involved tweaking conventional SVMs to weigh recent training instances more heavily than older instances. This is a particularly useful idea, since we face the same issue in our task: Recent posts are more descriptive of the current state of the thread, and hence should be more useful in predicting the next post.

% Min: You need a conclusion to this chapter.