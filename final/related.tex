\chapter{Related Work}
In this chapter, we present some of the work done related to prediction of 
thread updates. First, we take a look at the literature dealing with incremental 
crawlers and their policies for revisiting a web page. Some work has been done 
to try to predict how often page content is updated, with the aim of scheduling 
download times in order to keep a local database fresh. We then also take a 
brief look at some work dealing with content and review some work related 
to evaluation metrics.

 

% Min: this chapter needs A LOT OF WORK.  The work you review is piecemeal at
% best and does not represent a comprehensive survey of the work in the area.
% I would cut your losses and try to make a coherent related work chapter
% instead of including one (mostly irrelevant) work from various tangential
% areas.

\section{Refresh policies for incremental crawlers}

We first discuss the \emph{timeliness} of our crawler to maintain the freshness 
of the local database, which refers to how new the extracted information is. Web 
crawlers can be used to crawl sites for user comments for 
later post-processing. Web crawlers which maintain the freshness of a database of 
crawled content are known as incremental crawlers. Two trade-offs these crawlers 
face cited by \outcite{Yang2009} are \emph{completeness} and \emph{timeliness}.  
\emph{Completeness} refers to the extent which the crawler fetches all the 
pages, without missing any pages. \emph{Timeliness} refers to the efficiency 
with which the crawler discovers and downloads newly-created content. We focus 
mainly on timeliness in this project, as we believe that timely updates of 
active threads are more important than complete archival of all threads in the 
forum site.

Many such works have used the Poisson distribution to model page updates.  This 
is because the Poisson process is often used to model a sequence of events that 
happen independently, with an average rate, over time.  \outcite{Coffman1997} 
analysed the theoretical aspects of doing this, as well as formalised the 
problem. Their work showed that if the page change process is governed by a 
Poisson process with a rate of $\mu$, then accessing the page at intervals 
proportional to $\mu$ is optimal.

Cho and Garcia-Molina trace the change history of 720,000 web pages collected 
over four months, and showed empirically that the Poisson process model closely 
matches the update processes found in web pages \cite{Cho1999}. They then 
proposed different revisiting or refresh policies 
\cite{Cho2003,Garcia-molina2003} that attempt to maintain the freshness of the 
database.

The Poisson distribution were also used in \outcite{Tan2007}, where they 
described a method that made use of a weighted history, such that recent changes 
are more important than older ones. 

However, the Poisson distribution is memoryless, and in experimental results due 
to \outcite{Brian2000}, the behaviour of site updates are not. Moreover, these 
studies were not performed specifically on online threads, where the behaviour 
of page updates differs from static pages. In our own analysis of the dataset 
that we have extracted, updates to forum threads are also not independent of the 
day of the week or hour of the day (See Chapter 5).

\outcite{Yang2009}, attempted to resolve this by using the list structure of 
forum sites to infer a sitemap. With this, they reconstruct the full thread, and 
then use a linear-regression model to estimate a scoring function. This function 
is then used to schedule when the next visit to the thread will be made.  

Forums have a logical, hierarchical structure in their layout, which typically 
alerts the user to thread updates by putting threads with new replies at the top 
of the thread index. Yang's work exploits this as well as their linear model to 
achieve a prediction of when to retrieve the pages.  However this design pattern 
is not applied universally; comments on blog sites or e-commerce sites about 
products do not conform to this pattern.  The lack of such information may 
result in a poorer estimate, or no estimate at all.

What we are trying to do here is similar. We also want to create a revisit 
policy, but we argue that the previous rates of change should not be the only 
factor that is taken into account when coming up with such a policy. The prior 
work presented above all try to estimate the arrival of the next update or 
comment, but do not leverage an obvious source of information, which is the 
content of the posts themselves. Our perspective is that the available thread 
content can be used to provide a better estimation for predicting page updates. 

Next, we look at some of the related work pertaining to thread content.

\section{Thread content analysis}
While there is little existing work using content to predict page updates, we 
review existing work related to analysing thread-based pages. We think such work 
will aid our efforts in content-based update prediction.

% Min: not clear what you mean by "links".  Need to elaborate.  Is this relevant?
\outcite{Wang2011} find hierarchical links between forum posts using lexical 
chaining.  They proposed a method to link posts using the tokens in the posts 
called $Chainer_{SV}$. While they analyse the contents of individual posts, the 
paper does not make any prediction with regards to newer posts.

There has also been some work done recently in predicting events in social 
media, and in particular, tweets.  \outcite{Wang} dealt with predicting the 
retweetability of tweets using content. They applied two levels of 
classification, the first level categorising tweets into 6 different types: 
Opinion, Update, Interaction, Fact, Deals and Others. This was done using 
similar techniques as \outcite{Sriram2010} and \outcite{Naaman2010}. The Opinion 
and Update categories are then further categorised into another three and two 
sub-categories each. The authors performed this categorisation using labeled 
Latent Dirichlet Allocation. These classifications are then used as features to 
predict the retweetability of a tweet. 

While this work leverages content to make predictions about user responses, the 
predictions are made into three classes: no retweets, low retweets, and high 
retweets. While this may not be directly useful, some of the features used in 
their work could be leveraged to make a more accurate prediction.


%Kleinberg used Hidden Markov Models to predict ``bursts" in message arrival 
%times \cite{Kleinberg2003}. In his running example, he used email messages, and 
%used time between messages to estimate the states that produced the sequence.  
%While the model may be able to predict what the state is for the next time 
%interval, it does so using the history of message arrival times, and does not 
%take into account the content within the messages themselves.


%One also cannot ignore the fact that social factors play a role when users 
%interact in an online discussion. Granovetter's threshold model for social 
%behaviour may also be useful in describing how the users behave as a whole.

\section{Evaluation metrics}
\outcite{Yang2009} proposed a metric for our particular problem of thread update 
prediction. Known as the $T$-score, it gives the average time difference between 
when a post is made and when the post is retrieved by a crawler. The lower the 
$T$-score, the better the model. However, the metric does not penalize for 
visits which retrieve nothing new from the thread.  As such, a crawler that 
repeatedly crawls the site at a frequent rate would do very well.

Broadening our search for more relevant evaluation metrics that take such
wasted bandwidth into account, we turn to related work in the evaluation of
segmentation algorithms.  In \outcite{Georgescul2009}, the authors propose a
new scheme for evaluating segmentation algorithms, $Pr_{error}$. This metric is 
the weighted sum of two probability counts $Pr_{fa}$ which is the probability 
that a false alarm segmentation is made, and $Pr_{miss}$ which is the 
probability that a segmentation is not made when there should be one.  
Unfortunately for our purposes, the metrics are calculated using the number of 
ground truths and predicted segmentations given a window. As such, it does not 
account for the ``distance" between the ground truths and the predicted 
segmentation.  It also does not allow for the predictions to appear after the 
ground truths, which are all requirements needed for a metric to evaluate 
timeliness of a model.

The metric proposed here, do not penalise predicted segmentations that come 
before the ground truths. However, we build on similar ideas to create a that is 
suitable metric for evaluating our methods (See Chapter 4).

\section{Conclusion}
% Jesse: this chapter feels a little lacking. What are the main arguments that
% you want to make regarding the related work for your thesis? For example (Im
% not sure if this is true):
% 1. That there is very little work that utilizes content features to do
% update prediction.
% --> If this is the case, then you need to mention all the works on update
% prediction, discuss what features they used, how well they work and then
% emphasize that they have not used content features
% 2. That there is no work that explicitly deals with thread update
% prediction.
% --> If thats the case, then you need to mention exactly what problems have
%  been tackled in the literature and how they are different or won't solve
%  your exact problem or how their solution is similar to yours.
% Jesse: Remember that your job as a writer is to communicate to your reader.
% What is it that you want your reader to know by reading this chapter? A
% book-full of facts without a clear message is no book.
The state of current work related to revisitation policies mainly use 
estimations of previous update intervals to predict future update times.  
Analysis of user-generated content also does not tackle the problem of 
predicting when new content is created or published. The current literature
pertaining to evaluation metrics is also lacking in what we require for our 
project.

These are the issues we will tackle with our work. We aim to use the existing content available in the thread to train models for 
predicting when future posts will arrive. In the next chapter, we take a look at 
the various methods we propose for tackling this problem of revisitation.
