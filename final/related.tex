% Min: you need a paragraph at the beginning to describe what topics
% (subsections) you're going to talk wbout and why.
In order to devise such an algorithm, we need to predict how often any user may 
update a page. Some work has been done to try to predict how often page content 
is updated, with the aim of scheduling download times in order to keep a local 
database fresh.

% Min: this chapter needs A LOT OF WORK.  The work you review is piecemeal at
% best and does not represent a comprehensive survey of the work in the area.
% I would cut your losses and try to make a coherent related work chapter
% instead of including one (mostly irrelevant) work from various tangential
% areas.

\section{Refresh policies for incremental crawlers}

We first discuss the \emph{timeliness} of our crawler to maintain the freshness 
of the local database, which refers to how new the extracted information is. Web 
crawlers can be used to crawl sites for user comments for 
later post-processing. Web crawlers which maintain the freshness of a database of 
crawled content are known as incremental crawlers. Two trade-offs these crawlers 
face cited by \outcite{Yang2009} are \emph{completeness} and \emph{timeliness}.  
\emph{Completeness} refers to the extent which the crawler fetches all the 
pages, without missing any pages. \emph{Timeliness} refers to the efficiency 
with which the crawler discovers and downloads newly-created content. We focus 
mainly on timeliness in this project, as we believe that timely updates of 
active threads are more important than complete archival of all threads in the 
forum site.

Many such works have used the Poisson distribution to model page updates.  
% Min: why?  Why is it a good fit for the model?
\outcite{Coffman1997} analysed the theoretical aspects of doing this, showing 
that if the page change process is governed by a Poisson process 
$\frac{\lambda^k e^{-\lambda \mu}}{k!}$, then accessing the page at intervals 
proportional to $\lambda$ is optimal.

Cho and Garcia-Molina trace the change history of 720,000 web pages collected 
over four months, and showed empirically that the Poisson process model closely 
matches the update processes found in web pages \cite{Cho1999}. They then 
proposed different revisiting or refresh policies 
\cite{Cho2003,Garcia-molina2003} that attempt to maintain the freshness of the 
database.

The Poisson distribution were also used in \outcite{Tan2007} and 
\outcite{Wolf2002}. %elaborate!!!!
% Min: agreed with the above comment.  Why are these works worth mentioning?  How do they inform your work?
However, the Poisson distribution is memoryless, and in experimental results due 
to \outcite{Brian2000}, the behaviour of site updates are not. Moreover, these 
studies were not performed specifically on online threads, where the behaviour 
of page updates differs from static pages.

\outcite{Yang2009}, attempted to resolve this by using the list structure of 
forum sites to infer a sitemap. With this, they reconstruct the full thread, and 
then use a linear-regression model to predict when the next update to the thread 
will arrive. 
%elaborate!!!
% Min: agreed.  Your description is about the technical, engineering details and not about the qctual work wrt to algorithms

Forums have a logical, hierarchical structure in their 
layout, which typically alerts the user to thread updates by putting threads 
with new replies at the top of the thread index. Yang's work exploits this 
as well as their linear model to achieve a predicton of when to retrieve the
pages.  However this design pattern is not applied universally; 
% Min: sorry broke your sentence.  Can you fix it?  I've tried.
comments on blog sites 
or e-commerce sites about products do not conform to this pattern.  The lack of such 
information may result in a poorer estimate, or no estimate at all.

The above works all try to estimate the arrival of the next update (comment), but do not leverage an obvious source of information, which is the content of the posts themselves. Our perspective
is that the available thread content can be used to provide a better estimation for predicting page updates. 

Next, we look at some of the related work
pertaining to thread content.

\section{Thread content analysis}
While there is little existing work using content to predict page updates, we 
review existing work related to analysing thread-based pages.  We 
think such work will aid our efforts in content-based update prediction.

% Min: not clear what you mean by "links".  Need to elaborate.  Is this relevant?
\outcite{Wang2011} find links between forum posts using 
lexical chaining. They proposed a method to link posts using the tokens in the 
posts called $Chainer_{SV}$. While they analyse the contents of individual 
posts, the paper does not make any prediction with regards to newer posts. The 
methods used to produce a numeric similarities between posts may be used as a 
feature to describe a thread in its current state, 
% Min: this isn't clear.  You haven't even described what your current model is, so to say it's non-trivial doesn't seem justified.
but incorporating this into 
our model is non-trivial.

There has been some work done recently in predicting events in social media, and 
in particular, tweets.  \outcite{Wang} dealt with predicting the retweetability 
of tweets using content. They applied two levels of classification, the first 
level categorising tweets into 6 different types: Opinion, Update, Interaction, 
Fact, Deals and Others. This was done using similar techniques as 
\outcite{Sriram2010} and \outcite{Naaman2010}. The Opinion and Update categories 
are then further categorised into another three and two sub-categories each. The 
authors performed this categorisation using labeled Latent Dirichlet Allocation.



%Kleinberg used Hidden Markov Models to predict ``bursts" in message arrival 
%times \cite{Kleinberg2003}. In his running example, he used email messages, and 
%used time between messages to estimate the states that produced the sequence.  
%While the model may be able to predict what the state is for the next time 
%interval, it does so using the history of message arrival times, and does not 
%take into account the content within the messages themselves.


%One also cannot ignore the fact that social factors play a role when users 
%interact in an online discussion. Granovetter's threshold model for social 
%behaviour may also be useful in describing how the users behave as a whole.

%% Min: You need a conclusion to this chapter.
\section{Conclusion}
The state of current work related to revisitation policies mainly use 
estimations of previous update intervals to predict future update times.  
Analysis of user-generated content also do not tackle the problem of predicting 
when new content is created or published. These are the issues we will tackle 
with our work.

We aim to use the existing content available in the thread to train models for 
predicting when future posts will arrive. In the next chapter, we take a look at 
the various methods we propose for tackling this problem of revisitation.
