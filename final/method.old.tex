\newcommand{\vocab}{\mathbf{v}}
\newcommand{\dtvec}{\mathbf{t}_\Delta}
\newcommand{\ctxvec}{\mathbf{t}_\text{ctx}}
\newcommand{\dt}{\Delta_t}
\newcommand{\prerror}{Pr_{error}}
\newcommand{\weights}{\mathbf{w}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\post}{\rho}
\renewcommand{\t}{t}
\newcommand{\w}{w}
% Jesse: you need to assume that you already have talked about the goals of
% the project in the introduction and/or in a transitionary paragraph at the
% end of the previous section. So this paragraph should read differently.
We aim to predict the amount of time between the arrival of the next post and 
the time the last post in the thread was made. The information available to us 
are the previously made posts that we observe when first visiting the thread.  
The assumption made here is that the thread is not paginated in any way, and a 
single visit to the thread gives us the latest posts without having to traverse 
through the links to the latest page. This is because in practice, we would be 
able to keep track of where the last visited page of the thread was, and reading 
the new posts would incur a few more requests to the thread. This, in comparison 
to constantly hitting the page for updates, would be negligible.

% Jesse: you are not compiling your LaTeX document correctly. The table
% number does not appear in the pdf.
% Jesse: Even though you defined the annotations in the table, you need to
% mention them inline as well. For example, "Given the t-th post p ...".
% Jesse: Up to this point you have not mentioned any notion of a window. You
% shouldn't just make the concept appear in the formal definition without
% explanation or motivation.
More formally, what we are trying to do is to estimate a function $f$ such that 
given a feature vector $\X$ representative of a window $\post_{t - w + 
1},\post_{t - w + 2},\cdots ,\post_t$, we can approximate $\dt$ with $f(\X)$. In 
the following sections, we will discuss various methods for estimating $f$.  
Various notations will be used, a quick reference is provided in Table 
\ref{table:notations}.

\begin{table}
	\begin{center}
	\begin{tabular}{l l}
	\hline
Notation	&	Description\\
	\hline
$\post$		&	A post\\
$\t$		&	Index of a post in a thread\\
$\w$		&	Number of posts in a window\\
$\post_\t$	&	The $t$-th post in the thread\\
$\vocab_\t$	&	The frequency count vector of the posts used in the $t$-th 
	post\\
$\dt$		&	Time difference between a post at position $\t$ and a post at 
	position $\t+1$ \\
$\dtvec$	&	Vector of $\dt$s in a given window\\
$\ctxvec$	&	Bit vector representing the day of week, and the hour of day\\
$\X$		&	Feature vector extracted from a window\\
	\hline
	\end{tabular}
\end{center}
\caption{Notation reference} \label{table:notations}
\end{table}

%We extracted the timestamp, author and text content for each post in each thread
%from the forums. Our dataset was obtained from from \url{avsforums.com}.
%
%\begin{description}
%	\item[Previous time differences] All the time differences between posts made 
%		in the window. ($\dtvec$)
%	\item[Time-based features] Day of week, Hour of day. Provides contextual 
%		information about when the post was made. ($\ctxvec$)
%	\item[Content features (text)] Word frequency counts are used for this set 
%		of experiments. Using regression, we find the top $K$ variables that the 
%		actual $\dt$ depends on.  Table \ref{vocab_exp} reflect the results of 
%		the experiments done with varying values of $K$.
%\end{description}
%


In the following sections, we describe the various methods we have to 
approximate $f$.

\section{Baselines}
A simple way of estimating the revisit rate would be to use the average time 
differences given the observed posts, or a training set. In previous work, we 
have seen that if page updates follow a Poisson distribution, then revisiting at 
the Poisson mean would be an optimal revisit policy. %cite paper.

There are some details that need to be considered. One revisit policy would be 
to do so at a constant, fixed rate, independent of the posts being made to the 
thread. 
% Jesse: does not compute. If you're trying to explain a method here, it does
% not show.
For our baseline revisit policy, we took into account the last made post 
whenever we revisit, and calculate our next revisit time based on average post 
intervals added to the timestamp of the previous post.

% Jesse: Use a consistent terminology. Have you used "post interval timing"
% before?
% Jesse: you could do a better job at explaining the sliding window
% methodology. Can you cite a paper instead?
Another way of predicting using average post intervals would be to use a sliding 
window. Averaging out the time differences between the posts would intuitively 
work, because it captures the context of the situation: A series of posts with 
short intervals should mean that the next post would come at around the same 
interval as the few that came before.

%TODO:Rewrite.
In terms of using content for prediction, windows should also make sense 
because:
people view content as paginated posts, so time differences between posts don't 
really affect people's decision to post or not. Rather, reading the posts together
affect whether or not the user chooses to reply.

\begin{figure}
	\begin{center}
	\input{diagrams/eventsdemo.tex}
	\caption{%
A series of events, posts (blue) and visits (orange).  The diagram demonstrates 
the concept of a window of $w=2$.
}\label{fig:event_series}
	\end{center}
\end{figure}

\section{Performing regression on windows}
% Jesse: I dont understand what role this paragraph plays. It doesnt seem to
% add to explaining your method, which up to this point, still has not been
% explained. 
Previous work has used linear regression on a number of different features 
extracted from forums \cite{Yang2009}. In their paper, the regressed function 
was used as a scoring function rather than a predictive function. In site of 
this, we attempted to implement the same model, but this resulted in evaluations 
worse than that of the baseline.

% Jesse: So are we discussing about features now? Why is this under the
% "Performing regression on windows" section? 
We did use some of the features mentioned in the paper: Window posts time 
differences and time context features (bit-vector representations of the day of 
the week and hour of the day). In our own statistics we took from the 
\url{avsforum.com} threads, 
% Jesse: How did you utilize this finding? Is it manifested in your method
% somehow? What method?  
we have also found that the time of the day the day 
of the week matters when dealing with threads. An example of such a thread can 
be seen in Figure \ref{fig:hr_freq} and Figure \ref{fig:week_freq}, where it can 
be seen that activity on the board is highest at 2 PM, and drops slightly, 
suggesting some type of lunch period, and then goes up again during the early 
evening and at 9 PM, before dropping to its lowest at 3 AM. The weekly graph 
also shows a pattern, showing lower posting frequencies during the weekends, and 
its highest during Thursdays.

\begin{figure}
\begin{center}
\begin{subfigure}[b]{0.8\textwidth}
\includegraphics[width=\textwidth]{diagrams/hoursofday.png}
\caption{The hourly post frequency for the hours during the day.}
\label{fig:hr_freq}
	\end{subfigure}
	\begin{subfigure}[b]{0.8\textwidth}
\includegraphics[width=\textwidth]{diagrams/daysofweek.png}
\caption{The daily post frequency for the hours during the week. (Monday is 0)}
\label{fig:week_freq}
	\end{subfigure}
\end{center}
\end{figure}

However, this time in stead of linear regression, we used a regression method 
known as Support Vector Regression (SVR). This method allows the using of 
different kernels, allowing for better estimation of the target function.

% Jesse: Are you just naming the possible features or are these the actual
% features used by your method? Nothing is clear.
The main focus of study in this report was to see if content helps with 
predicting thread updates would produce an improvement. Some of the ways that 
content data were extracted into feature vectors are the following: Word 
frequency, the tf-idf of these words, and Part-of-Speech tags.

We perform the standard preprocessing steps like removing stopwords and tokens 
of length less than three. We also use Porter's stemming algorithm as another 
preprocessing step, before performing a word frequency count. However, the use 
of the full vocabulary of the thread as a feature vector greatly increases the 
time needed to train the model. As such, we used a simple univariate regression 
technique for feature selection, and selected only the $K$ best tokens for 
consideration. Table \ref{table:vocab_exp} shows the results of this experiment.  

\begin{table}
	\footnotesize
	\begin{centering}
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
	\hline
	\input{tables/vocab_exp}
	\hline
	\end{tabular}
	\caption{Experiment results: Varying vocabulary size}
	\label{table:vocab_exp}
\end{centering}
\end{table}


% Jesse: I dont see K in the table. How did you determine K? What is it?
These feature sets are used in different combinations, with different window 
sizes. The results will be seen in the next chapter.

\section{Discounted sum of previous instances}
The current method uses only information on the current $w$ posts. However, 
posts made further in the history of the thread may have an effect on when the 
latest posts arrive. The magnitude of this effect, however, may diminish over 
time.

% Jesse: "attempt to use"? So you planned to use this function, but failed?
Following this intuition we used a discounted sum over previous posts' word 
frequency vector:
\[
	\X'_t = \X_t + \alpha \X'_{t-1}
\]
where $\X_t$ is the feature vector at post $t$, and $\vocab$ is the word 
frequency vector. $\alpha$ is the \emph{discount factor} and satisfies $0 \leq 
\alpha < 1$.

\section{Stochastic Gradient Descent}
%TODO: describe why conventional SGD doesn't work.
We also used stochastic gradient descent to estimate the function $f$.  However, 
during runtime, instead of using a static function, we continue to allow $f$ to 
vary whenever new posts and their update times are observed.
Since $f(\X_{t-w},\hdots,\X_{t-1}) > 0$, we used a scaled sigmoid function,
\[
	f(\X) = \frac{\Lambda-\lambda}{1 + e^{\weights \cdot \X}} + \lambda
\]
where $\Lambda$ and $\lambda$ are the scaling factors. This results in $f: 
\mathbb{R}^{|\X|}  \rightarrow (\lambda,\Lambda)$. Bounding the estimation 
function between $\lambda$ and $\Lambda$ allows us to restrict the prediction 
from becoming negative, or, becoming exceedingly huge. For our purposes, we set 
$\lambda = Q_3 + 2.5(Q_{3} - Q_{1})$, where $Q_n$ is the value at the $n$-th 
quartile. 

The resulting update rule for $\weights$ is then given by,
\[
	\Delta \weights_i = \eta
				\underbrace{\left(\widehat{\dt} - \dt \right)}_{\text{error term}}
				\underbrace{\left( f(\X)(1-f(\X)) \right)}_{\text{gradient}}
						\X_i
\]
which is similar to the delta update rule found in artificial neural networks.  
We omit the scaling factor in the gradient as it is a constant and then 
experiment with various values of $\eta$, the learning rate. 

% Jesse: your method chapter is confusing. You are explaining bits and pieces
% of your method without actually taking time to define, explicitly, what your
% method entails and what motivates this approach. This chapter so far feels
% like you're explaining in great detail about every piece of a jigzaw puzzle
% without actually showing me what the completed puzzle looks like.
