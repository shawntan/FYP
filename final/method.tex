\newcommand{\vocab}{\mathbf{v}}
\newcommand{\dtvec}{\mathbf{t}_\Delta}
\newcommand{\ctxvec}{\mathbf{t}_\text{ctx}}
\newcommand{\dt}{\Delta_t}
\newcommand{\prerror}{Pr_{error}}
\newcommand{\weights}{\mathbf{w}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\post}{\rho}
\renewcommand{\t}{t}
\newcommand{\w}{w}

% Min: tip - Move newcommands to the report.tex file so that they can be globally available.

% Min: still a bit verbose.  Try to make your prose exact.  
We aim to predict the amount of time between the arrival of the next upcoming post and 
the time the last post in the thread was made. The information available to us 
are the previous posts observed in prior visits.  
The assumption made here is that the thread is not paginated in any way, and a 
single visit to the thread gives us the latest posts without having to traverse 
through the links to the latest page. This is because in practice, we would be 
able to keep track of where the last visited page of the thread was, and reading 
the new posts would incur a few more requests to the thread. This, in comparison 
to constantly hitting the page for updates, would be negligible.

More formally, what we are trying to do is to estimate a function $f$ such that 
given a feature vector $\X$ representative of a window $\post_{t - w + 
1},\post_{t - w + 2},\cdots ,\post_t$, where $\post_t$ represents the $t$-th 
post in the thread, we can approximate $\dt$ with $f(\X)$.  In the following 
sections, we will discuss various methods for estimating $f$, following the notation introduced in Table \ref{table:notations}.

\begin{table}
	\begin{center}
	\begin{tabular}{l l}
	\hline
Notation	&	Description\\
	\hline
$\post$		&	A post\\
$\t$		&	Index of a post in a thread\\
$\w$		&	Number of posts in a window  \\
$\post_\t$	&	The $t$-th post in the thread\\
$\vocab_\t$	&	The frequency count vector of the posts used in the $t$-th 
	post\\
$\dt$		&	Time difference between a post at position $\t$ and a post at 
	position $\t+1$ \\
$\dtvec$	&	Vector of $\dt$s in a given window\\
$\ctxvec$	&	Bit vector representing the day of week, and the hour of day\\
$\X$		&	Feature vector extracted from a window\\
$K$			&	The $K$ best features selected from the vocabulary.\\
	\hline
	\end{tabular}
\end{center}
\caption{Common notation used throughout this thesis} \label{table:notations}
\end{table}

We now describe the methods we explored to approximate $f$.

\section{Baselines}
A simple way of estimating the revisit rate is to use the average time 
difference given past observed posts. In our review of the related work, we 
have seen that if page updates follow a Poisson distribution, then revisiting at 
the Poisson mean would be an optimal revisit policy. %cite paper

% Min: you should refer to your work consistently.  Is it a policy, an algorith, an approximation, or a strategy?  Use one and only one.
% Min: Show a graphical illustration of this first baseline.
In our baseline revisit policy, we account for the last made post whenever 
we make a visit to the thread, and calculate our next revisit time based on the 
average post intervals added to the timestamp of the previous post. This is in 
contrast to an even simpler revisit policy that just revisits at a constant, 
fixed rate, independent of the posts being made to the thread.

% Min: give an enumeration or code for each type of method you try (e.g., B1 baseline 1, M1 my method 1.)
One other way of predicting using average post intervals would be to use the 
concept of a \emph{window}. Averaging the time differences between the posts 
intuitively works as it captures the thread's context: A 
series of posts with short intervals should mean that the next post would come 
at around the same interval as the few that came before.

% Min: why are you discussing content here?  The baselines don't use content.  Please fix.
% Min: you're introducing windows for the first time here, so perhaps you need to first explain what windows are.
In terms of using content for prediction, windows also make sense: Forum users 
view content as paginated posts, so time differences between posts do not affect 
% Min: why would pagination justify windows?  Be precise.
their decision to post. 
% Min: is this true?  Do you have any data or works to cite to back this assertion up?
Rather, reading a number of posts together affect 
whether or not the user chooses to reply.

An example of a window ($w=2$) can be seen in Figure \ref{fig:event_series}.

\begin{figure}
	\begin{center}
	\input{diagrams/eventsdemo.tex}
	\caption{%
% Min: also try to use textures in addition to color, for anyone viewing your report in B/W.  
A series of events, posts (blue) and visits (orange).  The diagram demonstrates 
the concept of a window of $w=2$.
}\label{fig:event_series}
	\end{center}
\end{figure}


\section{Performing regression on windows}
% Jesse: I dont understand what role this paragraph plays. It doesnt seem to
% add to explaining your method, which up to this point, still has not been
% explained.
% Shawn: To explain why I didn't do simple linear regression.
Previous work used linear regression on a set of features 
extracted from forums \cite{Yang2009}. In this past work, the regressed function 
% Min: show results or point forward to them.
was used as a scoring function rather than a predictive function. We implemented their model, but the results proved to perform worse than the baselines.

Informed by the past work, we use some features from the previous work: Window posts time 
differences and time context features (bit-vector representations of the day of 
the week and hour of the day). 
% Min: this doesn't go here.  Dataset information goes in a separate part of the report, in the evaluation section.
In our own statistics, we took from the 
\url{avsforum.com} threads, we have also found that the time of the day the day 
of the week matters when dealing with threads. An example of such a thread can 
be seen in Figure \ref{fig:hr_freq} and Figure \ref{fig:week_freq}, where we see that activity peaks at 2 PM, dropping slightly during (an assumed) lunch period, and then peaks again during the early 
evening and at 9 PM.  Activity drops to its lowest at 3 AM. The weekly graph 
also shows a pattern, showing lower posting frequencies during the weekends, and 
its highest peak on Thursdays.
\begin{figure}
\begin{center}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{diagrams/hoursofday.png}
\caption{The hourly post frequency.}
\label{fig:hr_freq}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{diagrams/daysofweek.png}
\caption{The daily post frequency. (Monday is 0)}
\label{fig:week_freq}
	\end{subfigure}
\end{center}
\end{figure}

% Min: this paragraph can be compacted with the previous paragraph.  Says nothing new.
This suggests that the \emph{time context} of which the posts were made are 
important when trying to determine the rate of posts.  As such, we factor in the 
day and hour information into our feature sets as well.

% Min: you need to organize your methods so that you can introduce each method and its accompanying features properly as well as the (machine) learning method.  Right now, it's still a mess of a narrative, not properly organized into different sections.
% Min: describe SVR and RBF, and why you chose them
However, this time instead of linear regression, we used a regression method 
known as Support Vector Regression (SVR), using a radial basis function kernel.  
This method allows the using of different kernels, allowing for better 
estimation of the target function.

% Min: describe the features.  You need to introduce tf*idf, and POS tags.  
The main focus of study in this report was to see if content helps with 
predicting thread updates would produce an improvement. Some of the ways that 
content data were extracted into feature vectors are the following: Word 
frequency, the tf-idf of these words, and Part-of-Speech tags.

We perform standard preprocessing steps of removing stopwords and tokens that are less than three characters in length. We also stem the words using the Porter stemmer.
We then perform a word frequency count. However, the use 
of the full vocabulary of the thread as a feature vector greatly increases the 
time needed to train the model.
% Min: give some evidence for the amount of time incurred and change of time incurred by this step.
 As such, we used a simple univariate regression 
technique for feature selection, and selected only the $K$ best tokens for 
consideration. Table \ref{table:vocab_exp} shows the results of this experiment.  
	
\begin{table}
	\footnotesize
\begin{center}
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
	\hline
	\input{tables/vocab_exp}
	\hline
	\end{tabular}
\end{center}
	\caption{Experiment results: Varying vocabulary size}
	\label{table:vocab_exp}
\end{table}

% Min: this says almost nothing except that things change.
These feature sets are used in different combinations, with different window 
sizes. The results will be seen in the next chapter.

The methods in this section use features extracted from the current window. A 
model is then trained using these extracted features in order to make a 
prediction. We take a look now at a two other novel methods that we developed.

\section{Discounted sum of previous instances}
 
Posts made further in the history of the thread may have an effect on when the 
latest posts arrive. The magnitude of this effect, however, may diminish over 
time.

Instead of having a finite window for which all posts (in said window) are 
treated equally, we propose to accord different weights for the previous posts: the earlier the posts were made, the less effect they should have on prediction 
the post should have.

Following this intuition we used a discounted sum over previous posts' word 
frequency vector:
\[
	\X'_t = \X_t + \alpha \X'_{t-1}
\]
where $\X_t$ is the feature vector at post $t$. $\alpha$ is the \emph{discount 
factor} and satisfies $0 \leq \alpha < 1$.

This new feature vector $\X'_t$ will be used in the same way as before; 
instances of $\X'$ will be regressed with their $\dt$ values. As before, we will 
look at the results for this method in the next chapter.

Up till now, we have looked at methods that treat the model as static -- once 
trained, the model never gets updated during run time. However, this is 
unrealistic due to the fact that over time, different words are popular as a 
direct result of different topics in the real world being popular. In this case, 
these fluctuations may be due to new updates to firmware being released or newer 
models of a product.

\section{Stochastic Gradient Descent}
To address this problem, we investigate the use of a stochastic gradient descent (SGD) to estimate the function $f$.  The SGD allows us to use a dynamic function instead of a static function for estimation 
during runtime.  We continue to allow $f$ to 
vary whenever new posts and their update times are observed.

% Min: try to write a bit more formally here.
Having already attempted using linear regression for this purpose, we have found 
it unsuitable for $f$ to be estimated by a linear function. Such a linear 
function often resulted in a negative prediction, and sometimes an 
overly huge one, when given feature vectors that have previously never been 
observed. The function has to be somehow constrained such that the value 
returned never drops below 0, and never predicts something too huge such that 
many posts are missed.

Since $f(\X) > 0$, we used a scaled sigmoid function,
\[
	f(\X) = \frac{\Lambda-\lambda}{1 + e^{\weights \cdot \X}} + \lambda
\]
where $\Lambda$ and $\lambda$ are the scaling factors. This results in $f: 
\mathbb{R}^{|\X|}  \rightarrow (\lambda,\Lambda)$. Bounding the estimation 
function between $\lambda$ and $\Lambda$ allows us to restrict the prediction 
from becoming negative or exceedingly large. For our purposes, we set 
$\lambda = Q_3 + 2.5(Q_{3} - Q_{1})$, where $Q_n$ is the value at the $n$-th 
quartile. A visual interpretation of such a curve can be seen in Figure 
\ref{fig:scaled_sigmoid}.
\begin{figure}
\begin{center}
\begin{tikzpicture}
	\begin{axis}[
		xlabel=$\weights\cdot\X$,
		ylabel=$f(\X)$,
		ytick={0,0.5,1},
		yticklabels={$\lambda$,$\frac{\lambda + \Lambda}{2}$,$\Lambda$}
	]
\addplot {1/(1+ e^-x)}; \end{axis}
\end{tikzpicture}
\end{center}
\caption{Scaled sigmoid curve}\label{fig:scaled_sigmoid}
\end{figure}

The resulting update rule for $\weights$ is then given by,
\[
	\Delta \weights_i = \eta
				\underbrace{\left(\widehat{\dt} - \dt \right)}_{\text{error term}}
				\underbrace{\left( f(\X)(1-f(\X)) \right)}_{\text{gradient}}
						\X_i
\]
which is similar to the delta update rule found in artificial neural networks.  
We omit the scaling factor in the gradient as it is a constant and then 
experiment with various values of $\eta$, the learning rate. 

In this chapter, we have outlined the specific task we will be attempting, to 
try and predict the time from the current last post in the thread to the next.
We have discussed the types of features we will be using, time context features, 
with a focus on content features that consists mainly of the tokens present. We 
have also discussed the concept of a window, and how it could help to make 
predictions better. Also, two novel methods were discussed, and, in the next 
chapter, we will look at how these methods stack up against one another.

