\chapter{Method}

% Min: still a bit verbose.  Try to make your prose exact.  
We aim to predict the amount of time between the arrival of the next upcoming 
post and the time the last post in the thread was made. The information 
available to us are the previous posts observed in prior visits.  The assumption 
made here is that the thread is not paginated in any way, and a single visit to 
the thread gives us the latest posts without having to traverse through the 
links to the latest page. This is because in practice, we would be able to keep 
track of where the last visited page of the thread was, and reading the new 
posts would incur a few more requests to the thread. This, in comparison to 
constantly hitting the page for updates, would be negligible.

More formally, what we are trying to do is to estimate a function $f$ such that 
given a feature vector $\X$ representative of a window $\post_{t - w + 
1},\post_{t - w + 2},\cdots ,\post_t$, where $\post_t$ represents the $t$-th 
post in the thread, we can approximate $\dt$ with $f(\X)$.  In the following 
sections, we will discuss various methods for estimating $f$, following the notation introduced in Table \ref{table:notations}.

\begin{table}
	\begin{center}
	\begin{tabular}{l l}
	\hline
Notation	&	Description\\
	\hline
$\post$		&	A post\\
$\t$		&	Index of a post in a thread\\
$\w$		&	Number of posts in a window  \\
$\post_\t$	&	The $t$-th post in the thread\\
$\vocab_\t$	&	The frequency count vector of the posts used in the $t$-th 
	post\\
$\dt$		&	Time difference between a post at position $\t$ and a post at 
	position $\t+1$ \\
$\dtvec$	&	Vector of $\dt$s in a given window\\
$\ctxvec$	&	Bit vector representing the day of week, and the hour of day\\
$\X$		&	Feature vector extracted from a window\\
$K$			&	The $K$ best features selected from the vocabulary.\\
	\hline
	\end{tabular}
\end{center}
\caption{Common notation used throughout this thesis} \label{table:notations}
\end{table}

We now describe the methods we explored to approximate $f$.
\section{Baselines}
A simple way of estimating the revisit rate is to use the average time 
difference given past observed posts. In our review of the related work, we 
have seen that if page updates follow a Poisson distribution, then revisiting at 
the Poisson mean would be an optimal approximation \cite{Coffman1997}.
% Min: Show a graphical illustration of this first baseline.
In our baseline revisit policy, we account for the last made post whenever 
we make a visit to the thread, and calculate our next revisit time based on the 
average post intervals added to the timestamp of the previous post. This is in 
contrast to an even simpler revisit policy that just revisits at a constant, 
fixed rate, independent of the posts being made to the thread.

% Min: give an enumeration or code for each type of method you try (e.g., B1 baseline 1, M1 my method 1.)
One other way of predicting using average post intervals would be to use the 
concept of a \emph{window}. Averaging the time differences between the posts 
intuitively works as it captures the thread's context: A series of posts with 
short intervals should mean that the next post would come at around the same 
interval as the few that came before.

An example of a window ($w=2$) can be seen in Figure \ref{fig:event_series}.

\begin{figure}
	\begin{center}
	\input{diagrams/eventsdemo.tex}
	\caption{%
% Min: also try to use textures in addition to color, for anyone viewing your report in B/W.  
A series of events, posts (blue) and visits (orange).  The diagram demonstrates 
the concept of a window of $w=2$.
}\label{fig:event_series}
	\end{center}
\end{figure}

% Min: why are you discussing content here?  The baselines don't use content.  Please fix.
% Min: you're introducing windows for the first time here, so perhaps you need to first explain what windows are.
In terms of using content for prediction, windows also makes intuitive sense: 
forum users view content as paginated posts, so whether or not they make a reply 
depends on the content they view when viewing the thread.  


\section{Performing regression on windows (\texttt{SVR})}
Previous work used linear regression on a set of features extracted from forums 
\cite{Yang2009}. In this past work, the regressed function 
% Min: show results or point forward to them.
was used as a scoring function rather than a predictive function. We implemented 
their model, but the results proved to perform worse than the baselines.

Informed by the past work, we use some features from the previous work: window 
posts time differences and time context features (bit-vector representations of 
the day of the week and hour of the day).

% Min: you need to organize your methods so that you can introduce each method 
% and its accompanying features properly as well as the (machine) learning 
% method.  Right now, it's still a mess of a narrative, not properly organized 
% into different sections.
However, instead of linear regression, we used a regression method known as 
Support Vector Regression (SVR). SVR is an extension of using Support Vector 
Machines for classification. Since support vector optimisation does not depend 
on the dimension of the input space, SVR will have advantages in high dimension 
feature vectors \cite{drucker1997}. We are also using a radial basis function 
kernel.

The main focus of study in this report was to see if content helps with 
predicting thread updates would produce an improvement. In this project we used 
word counts. We performed standard preprocessing steps of removing stopwords and 
tokens that are less than three characters in length. We also stem the words 
using the Porter stemmer.
We then perform a word frequency count. However, the use 
of the full vocabulary of the thread as a feature vector greatly increases the 
time needed to train the model.
% Min: give some evidence for the amount of time incurred and change of time incurred by this step.
 As such, we used a simple univariate regression 
technique for feature selection, and selected only the $K$ best tokens for 
consideration. Table \ref{table:vocab_exp} shows the results of this experiment.  
The methods in this section use features extracted from the current window. A 
model is then trained using these extracted features in order to make a 
prediction. We take a look now at two other novel methods that we developed.

\section{Discounted sum of previous instances (\texttt{DEC})}
 
Posts made further in the history of the thread may have an effect on when the 
latest posts arrive. The magnitude of this effect, however, may diminish over 
time.

Instead of having a finite window for which all posts (in said window) are 
treated equally, we propose to accord different weights for the previous posts: the earlier the posts were made, the less effect they should have on prediction 
the post should have.

Following this intuition we used a discounted sum over previous posts' word 
frequency vector:
\[
	\X'_t = \X_t + \alpha \X'_{t-1}
\]
where $\X_t$ is the feature vector at post $t$. $\alpha$ is the \emph{discount 
factor} and satisfies $0 \leq \alpha < 1$.

This new feature vector $\X'_t$ will be used in the same way as before; 
instances of $\X'$ will be regressed with their $\dt$ values. As before, we will 
look at the results for this method in the next chapter.

Up till now, we have looked at methods that treat the model as static -- once 
trained, the model never gets updated during run time. However, this is 
unrealistic due to the fact that over time, different words are popular as a 
direct result of different topics in the real world being popular. In our case 
of an Audio-Visual equipment forum, these fluctuations may be due to new updates 
to firmware being released or newer models of a product.

\section{Stochastic Gradient Descent (\texttt{SGD})}
To address this problem, we investigate the use of a stochastic gradient descent 
(SGD) to estimate the function $f$.  \texttt{SGD} allows us to use a dynamic 
function instead of a static function for estimation during runtime.  We 
continue to allow $f$ to vary whenever new posts and their update times are 
observed.

% Min: try to write a bit more formally here.
Having already attempted using linear regression for this purpose, we have found 
it unsuitable for $f$ to be estimated by a linear function. Such a linear 
function often resulted in a negative prediction, and sometimes an 
overly huge one, when given feature vectors that have previously never been 
observed. The function has to be somehow constrained such that the value 
returned never drops below 0, and never predicts something too huge such that 
many posts are missed.

Since $f(\X) > 0$, we used a scaled sigmoid function,
\[
	f(\X) = \frac{\Lambda-\lambda}{1 + e^{\weights \cdot \X}} + \lambda
\]
where $\Lambda$ and $\lambda$ are the scaling factors. This results in $f: 
\mathbb{R}^{|\X|}  \rightarrow (\lambda,\Lambda)$. Bounding the estimation 
function between $\lambda$ and $\Lambda$ allows us to restrict the prediction 
from becoming negative or exceedingly large. For our purposes, we set 
$\lambda = Q_3 + 2.5(Q_{3} - Q_{1})$, where $Q_n$ is the value at the $n$-th 
quartile. A visual interpretation of such a curve can be seen in Figure 
\ref{fig:scaled_sigmoid}.
\begin{figure}
\begin{center}
\begin{tikzpicture}
	\begin{axis}[
		xlabel=$\weights\cdot\X$,
		ylabel=$f(\X)$,
		ytick={0,0.5,1},
		yticklabels={$\lambda$,$\frac{\lambda + \Lambda}{2}$,$\Lambda$}
	]
\addplot {1/(1+ e^-x)}; \end{axis}
\end{tikzpicture}
\end{center}
\caption{Scaled sigmoid curve}\label{fig:scaled_sigmoid}
\end{figure}

The resulting update rule for $\weights$ is then given by,
\[
	\Delta \weights_i = \eta
				\underbrace{\left(\widehat{\dt} - \dt \right)}_{\text{error term}}
				\underbrace{\left( f(\X)(1-f(\X)) \right)}_{\text{gradient}}
						\X_i
\]
which is similar to the delta update rule found in artificial neural networks.  
We omit the scaling factor in the gradient as it is a constant and then 
experiment with various values of $\eta$, the learning rate. 

In this chapter, we have outlined the specific task we will be attempting, to 
try and predict the time from the current last post in the thread to the next.
We have discussed the types of features we will be using,
\begin{itemize}
	\item $\ctxvec$: time context features
	\item $\vocab$: content features, in particular, word counts
	\item $\dtvec$: $\dt$ of posts within the window
\end{itemize}
We have also discussed the concept of a window, and how it could help to make 
predictions better. The methods we will evaluate in the next chapter are 
following:
\begin{itemize}
	\item \texttt{BL}: Baseline method, using the average $\dt$ revisit 
strategy.
	\item \texttt{SVR}: Support vector regression, using feature vector 
extracted from the current window.
	\item \texttt{DEC}: Uses the discounted sum of previous instances as the 
current window.
	\item \texttt{SGD}: Uses the stochastic gradient descent method for 
prediction.
\end{itemize}
We will perform some experiments, and look at how these methods stack up against 
one another.
