\section{Experiment setup}
The first 75\% of the thread was used as training data, while the remaining 25\% 
was used as test data. We used Support Vector Regression for this regression 
task, employing a Radial Basis Function kernel as our learning algorithm. 

\begin{figure}
	\input{diagrams/exp_setup}
	\caption{Our experiment setup}\label{fig:exp_setup}
\end{figure}

\subsection{Parameter Tuning}
Before we begin performing experiments on the full dataset, we first tuned the 
machine learning algorithms using a sample of the forum threads. In the 
following experiments, the threads chosen from our extracted dataset are those 
with a 100 to 1000 posts. This amounted to 97 threads.
%TODO: may need to change once results are out.
In each of these experiments, we run the algorithm with different parameters, 
and use the optimal one in our final evaluation. 

\subsubsection{Window size}
Using a combination of feature sets, we experiment with different window sizes, 
$w = 1, 5, 10, 15$.

Performing the experiment using only the $\dt$ values within the window, we 
obtain the results found in Table \ref{tbl:par_tune_dt}. The results show that 
$w=15$ provide the best $T$-score. We must however, keep in mind that its 
Visit/Post ratio is the highest, but also has a higher standard error.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
		& $Pr_{error}$		  & $T$-score			   &	Visit/Post\\
\hline
	\input{tables/dt_vec_features}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_dt}
\end{table}

Using only the content, we perform the same experiment again. Since the size of 
the vocabulary is large, we select the $K = 50$ best tokens to consider using 
Univariate feature selection. This gives us the results in Table 
\ref{tbl:par_tune_content}. The best $T$-score here does not do as well as that 
in the previous experiment. However, it is interesting to note that, again, 
$w=15$ results in the best $T$-score.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
		& $Pr_{error}$		  & $T$-score			   &	Visit/Post\\
\hline
	\input{tables/dt_vec_features}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_content}
\end{table}

For our final experiment for tuning the window size, we combine the various 
feature sets together. We also include the time-context in this experiment, and 
we arrive at the results found in Table \ref{tbl:par_tune_comb}. Again, $w=15$ 
has the best $T$-score, but only with a slight improvement over our first 
experiment.

In any case, this suggests that $w=15$ may be the best window size. In the 
following experiments, this will be our $w$ value.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
		& $Pr_{error}$		  & $T$-score			   &	Visit/Post\\
\hline
	\input{tables/comb_vec_features}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_comb}
\end{table}


\subsubsection{Decay factor}
In our discounted sum method, we have to tune the $\alpha$ parameter. We search 
through 0.1 to 0.9 (inclusive) with 0.1 increments to find the best possible 
value for $\alpha$.  We used the combined set of features for this experiment.
The results are shown in Table \ref{tbl:par_tune_decay}.
\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
		& $Pr_{error}$		  & $T$-score			   &	Visit/Post\\
\hline
	\input{tables/alpha_decay}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_decay}
\end{table}

$\alpha=0.9$ performs the best, but its improvement over the rest of the values 
for $\alpha$ are not by much. Also, note that the $T$-scores do not defer much 
from the previous experiment, although there is a slight improvement.

\subsubsection{Learning rate for Stochastic Gradient Descent}

Because of the scaling factors applied to the sigmoid function, a small change 
in the exponent of $e$ results in huge fluctuations. As such, we need to find a 
small enough learning rate such that the predicted values do not end up at only 
the extremes, but large enough such that the model is adaptive enough to 
``react" to changes.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
		& $Pr_{error}$		  & $T$-score			   &	Visit/Post\\
\hline
	\input{tables/learning_rate}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_learning}
\end{table}

In this experiment, we find that $\eta = \eta=5\cdot10^{-8}$ is the best value 
for the learning rate. Also note that this model produces the best results for 
the sample dataset.

In the following section, we will describe the experiment performed on the full 
dataset.

\section{Experiments}
