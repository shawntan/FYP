In our project, the dataset we used was crawled from 
\url{http://www.avsforum.com/f/}. The forum dealt mainly with Audio-Visual 
equipment, with discussions mainly about technical details, offers and people 
showing off their DIY projects.

The forum was chosen from the list which \outcite{Yang2009} provided in their 
paper. The forum users use mainly proper English, which made removing stopwords 
and stemming simpler.

We crawled 4,158 threads, with a total of 1,002,225 posts. A distribution of how 
the length of threads are distributed can be seen in Figure \ref{fig:len_dist}.  
Threads with 1 to 10 posts already make up half the number of collected threads.
The distribution of the time differences are shown in Figure 
\ref{fig:time_dist}. In both the figures, the right-hand-side cutoff was set at 
1,000 due to the negligible number of items to the right of the cutoff.


\putgraphic{diagrams/len_dist.png}{Distribution of thread 
length}{len_dist}{0.75}
\putgraphic{diagrams/time_dist.png}{Distribution of $\dt$}{time_dist}{0.75}

\section{Experiment setup}
The first 75\% of the thread was used as training data, while the remaining 25\% 
was used as test data. We used Support Vector Regression for this regression 
task, employing a Radial Basis Function kernel as our learning algorithm. 

\begin{figure}
	\input{diagrams/exp_setup}
	\caption{Our experiment setup}\label{fig:exp_setup}
\end{figure}

\subsection{Parameter Tuning}
Before we begin performing experiments on the full dataset, we first tuned the 
machine learning algorithms using a sample of the forum threads. In the 
following experiments, the threads chosen from our extracted dataset are those 
with a 100 to 1000 posts. This amounted to 97 threads.
%TODO: may need to change once results are out.
In each of these experiments, we run the algorithm with different parameters, 
and use the optimal one in our final evaluation. 

\subsubsection{Vocabulary size}
%TODO in progress.
\begin{table}
	\footnotesize
\begin{center}
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
	\hline
& $T$-score			   &	Visit/Post & 	$\prerror$\\
	\hline
	\input{tables/vocab_exp}
	\hline
	\end{tabular}
\end{center}
	\caption{Experiment results: Varying vocabulary size}
	\label{table:vocab_exp}
\end{table}



\subsubsection{Window size}
Using a combination of feature sets, we experiment with different window sizes, 
$w = 1, 5, 10, 15$.

Performing the experiment using only the $\dt$ values within the window, we 
obtain the results found in Table \ref{tbl:par_tune_dt}. The results show that 
$w=15$ provide the best $T$-score. We must however, keep in mind that its 
Visit/Post ratio is the highest, but also has a higher standard error.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
& $T$-score			   &	Visit/Post & 	$\prerror$\\
\hline
	\input{tables/dt_vec_features}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_dt}
\end{table}

Using only the content, we perform the same experiment again. Since the size of 
the vocabulary is large, we select the $K = 50$ best tokens to consider using 
Univariate feature selection. This gives us the results in Table 
\ref{tbl:par_tune_content}. The best $T$-score here does not do as well as that 
in the previous experiment. However, it is interesting to note that, again, 
$w=15$ results in the best $T$-score.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
& $T$-score			   &	Visit/Post & 	$\prerror$\\
\hline

	\input{tables/dt_vec_features}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_content}
\end{table}

For our final experiment for tuning the window size, we combine the various 
feature sets together. We also include the time-context in this experiment, and 
we arrive at the results found in Table \ref{tbl:par_tune_comb}. Again, $w=15$ 
has the best $T$-score, but only with a slight improvement over our first 
experiment.

In any case, this suggests that $w=15$ may be the best window size. In the 
following experiments, this will be our $w$ value.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
& $T$-score			   &	Visit/Post & 	$\prerror$\\
\hline
\input{tables/comb_vec_features}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_comb}
\end{table}


\subsubsection{Decay factor}
In our discounted sum method, we have to tune the $\alpha$ parameter. We search 
through 0.1 to 0.9 (inclusive) with 0.1 increments to find the best possible 
value for $\alpha$.  We used the combined set of features for this experiment.
The results are shown in Table \ref{tbl:par_tune_decay}.
\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
& $T$-score			   &	Visit/Post & 	$\prerror$\\
\hline
	\input{tables/alpha_decay}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_decay}
\end{table}

$\alpha=0.9$ performs the best, but its improvement over the rest of the values 
for $\alpha$ are not by much. Also, note that the $T$-scores do not defer much 
from the previous experiment, although there is a slight improvement.

\subsubsection{Learning rate for Stochastic Gradient Descent}

Because of the scaling factors applied to the sigmoid function, a small change 
in the exponent of $e$ results in huge fluctuations. As such, we need to find a 
small enough learning rate such that the predicted values do not end up at only 
the extremes, but large enough such that the model is adaptive enough to 
``react" to changes.

\begin{table}
\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
& $T$-score			   &	Visit/Post & 	$\prerror$\\
\hline
	\input{tables/learning_rate}
\hline
\end{tabular}
\end{center}
\caption{Some results}\label{tbl:par_tune_learning}
\end{table}

In this experiment, we find that $\eta=5\cdot10^{-8}$ is the best value for the 
learning rate. Also note that this model produces the best results for the 
sample dataset.


So at the end of tuning our feature set and parameters, we have the following 
set of parameters: $K = 50, w = 15, \alpha = 0.9, \eta = 5\cdot10^{-8}$. Using 
these parameters, we run a full evaluation on our dataset.

\section{Experiments}

Results of running Stochastic Gradient Descent
Results of running SVR
Results of running Decay


Analyses of the algo.




