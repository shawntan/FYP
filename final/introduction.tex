With the advent of Web 2.0, sites with forums, or similar thread-based
discussion features are increasingly common.  Our goal in this thesis
is to create an algorithm that can predict when updates in such
threads will occur.
\begin{table}
	\makebox[\textwidth][c]{
	{\footnotesize
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
		\hline
			\input{tables/web20}
		\hline
	\end{tabular}
	~\\
	}
	}
	{\footnotesize
\caption{Features of popular Web 2.0 sites}
\label{table:web20}
	\begin{tabular}{l l}
		T &= Twitter mentions\\
	 FB L &= Facebook Likes \\
		FB S &= Facebook Shares\\
	G +1 &= Google +1\\
		   L&= Likes (Local) \\
   		DL &= Dislikes (Local) \\
			C &= Comments \\
		PV &= Page Views \\
   Follows &= Site-local feature for keeping track of user's activities
	\end{tabular}
}
\end{table}

Table \ref{table:web20} shows us that many of the popular Web 2.0
sites have comment features. This suggests that content on the web is
increasingly being created by users alongside content providers.

%we are predicting web 2.0 updates
In an increasing number of cases, news travels more quickly through
online community discussions than through traditional media. Users also
typically discuss purchased products bought online on forums,
and companies that want to get timely feedback about their product
should turn to data mined from such sites.

Web crawling is largely IO-bound, a large portion of the time spent crawling is 
waiting for the server to supply a response to the request issued by the 
crawler. However, for sites with a large number of pages (like popular forum 
sites), make this infeasible in practice. On top of the usual requests it has, 
it then has to deal with repeated requests from such a crawler. Most sites do 
not mind some additional bandwidth, but if it gets excessive, it may be 
construed as a Denial-of-Service attack. At best, the site may deny any further 
requests from the crawler, and at worst the large number of requests may bring 
down the site.
% bandwidth use but too much is not a good thing.  See white-hat rules
% about spiders.

A simple method to reduce the amount of polling needed is to use the
average time differences between previous posts to estimate the
arrival of the next one, and to abstain from polling until the
estimated time.
% Min: you need to discuss how well this simple baseline does and whether it is 
% actually adopted.

A key observation in our work is that the contents of the thread may
also influence the discussion and hence the rate of commenting.  We
believe that the content of the thread has information that can give a
better estimate of the time interval between the last post and a new
one.

% Min: tie the ``hypothetical'' example with actual posts.  Also, your
% introduction needs to actually carry through to the final
% implementation and experimentation where you use such features to
% estimate properly.
For example, a thread in a technical forum about a Linux distribution may start 
out as a question. Subsequent questions that attempt to either clarify or expand 
on the original question may then be posted, resulting in a quick flurry of 
messages. Eventually, a more technically savvy user of the forum may come up 
with a solution, and the thread may eventually slow down after a series of 
messages thanking the problem solver. 

% Min: your introduction needs some work.
% Towards the end you need.  Please restructure the below.
% 1) a clear problem statement
% 2) statements of the contribution of your work
% 3) A navigation paragraph that describes how the rest of the report
% is going to be structured, especially if it deviates from ``normal''
% structure.
Let us define all such thread-based discussion styled sites as forums. Ideally, 
an incremental crawler of such user-generated content should be able to maintain 
a fresh and complete database of content of the forum that it is monitoring.  
However, doing so with the previously mentioned naive method would (1) incur 
excessive costs when downloading un-updated pages, and (2) raise the possibility 
of the web master blocking the requester's IP address.

Our high level goal: To come up with a suitable algorithm for revisiting user 
discussion threads, based on the discussion content in the thread. In this 
project, we focus on forum threads. We demonstrate three different methods for 
achieving this using regression methods, and also propose a new metric for 
measuring the timeliness of such a model that balances between the model's 
timeliness and bandwidth consumption.

In Chapter 2, we explore the related work dealing with predicting web page 
updates and metrics to measure the performance of such algorithms. In Chapter 3, 
we discuss the methods that we have come up with to tackle the problem, while 
Chapter 4 describes the metrics we propose for measuring the performance of 
revisitation algorithms. In Chapter 5, we perform experiments on a dataset 
extracted from \url{avsforum.com}, and show that our models perform better than 
an average revisitation baseline. Chapter 6 then discusses our contributions, 
and possible avenues of future work.
