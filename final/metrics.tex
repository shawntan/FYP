\renewcommand{\P}{Pr}
\chapter{Evaluation Metrics}


\begin{table}
\begin{center}
\begin{tabular}{l l}
	\hline
Notation	&	Description		\\
	\hline
$P$			&	List of posts. \\
$V$			&	List of visits.\\
$T$			&	A thread's $T$-score. \\
	$T_\text{max}$	&	A thread's maximum $T$-score. \\
$t(\post)$	&	Timestamp of the post.\\
	\hline
\end{tabular}
\end{center}
	\caption{Notation used for evaluation metrics}
\end{table}


One of the contributions of this project was also to come up with a good metric 
for measuring the performance of a model that performs predictions. 

In trying to do this, we must first consider the two constraints that we are 
trying to balance: (1) The bandwidth consumption of our algorithm, and (2) the 
timeliness of our predictions. A violation of either should incur a penalty in 
the metric we are using. Such a metric could also be parameterised so that 
different levels of importance could be given to (1) or (2), depending on the 
situation.

In the next section, we will discuss two metrics that can be used to measure 
this. The following section then demonstrates how the two metrics can be 
normalised, and then combined to form our final $\prerror$ metric.
\pagebreak
\section{$T$-score and the Visit/Post ratio}

\begin{figure}
	\begin{center}
	\input{diagrams/t_score_diag}
	\caption{Demonstrating how $T$-score is measured.}
	\end{center}
\end{figure}


We also want to know the \emph{timeliness} of the model's visits.  
\outcite{Yang2009} has a metric for measuring this. Taking $\Delta t_i$ as the 
time difference between a post $i$ and it's download time, the timeliness of the 
algorithm is given by
\[
	T = \frac{1}{|P|} \sum^{|P|}_{i=1}\Delta t_i
\]

A good algorithm would give a low $T$-score. However, a crawler that hits the 
site repeatedly performs well according to this metric. The authors account for 
this by setting a bandwidth (fixed number of pages per day) for each iteration 
of their testing. In our experimental results, we also take into account the 
number of page requests made in comparison to the number of posts. %ratio?


\section{Normalising the $T$-score and Visit/Post ratio}
We now have two different metrics that we need to combine into a single score to 
measure an algorithm's performance. In the same spirit as 
\outcite{Georgescul2009}, we normalise the $T$-score and Visit/Post ratio to get 
two values that we can use a weighted average to combine. In order to do this, 
we consider again the thread posts and visits as a sequence of events. Any 
visits that occur after the last post are ignored.

We then consider the worst case in terms of timeliness, or misses. This would be 
the case where the visit comes at the end, at the same time as the post. So we 
get a value $T_{\max}$ and $P_{\text{miss}}$ such that
\begin{align*}
	\P_{\text{miss}} &= \frac{T}{T_{\max}}\\
							  &= \dfrac{T}{\left(
					\dfrac{%
			\sum_\post (\max_{\post'}t(\post') - t(\post))}{|P|}\right)} \\
					&= \dfrac{|P| \cdot T}{%
		\sum_\post (\max_{\post'}t(\post') - t(\post))}
\end{align*}

\pagebreak
An example can be viewed in Figure \ref{fig:norm_t_score}. Assuming that there 
are no posts before $\post_1$ here, we simply take the usual $T$-score value to 
get $T_\text{max}$
\begin{figure}
\begin{center}
\input{diagrams/norm_t_score_diag}
	\end{center}
\caption{An example of calculating $T_\text{max}$. A visit is assumed at the 
same time as the final post made, and the usual $T$-score metric is 
calculated}\label{fig:norm_t_score}
\end{figure}
It is difficult to consider the worst case in terms of false alarms, or visits 
that retrieve nothing. There could be an infinite number of visits made if we 
are to take the extreme case. In order to get around this, we consider discrete 
time frames in which a visit can occur. Since for this dataset, our time 
granularity is in terms of minutes, we shall use minutes as our discrete time 
frame.

\pagebreak
Using this simplified version of our series of events, we can then imagine a 
worst-case performing revisit policy that visits at every single time frame.  
Here, we assume all quantities are measured in terms of minutes. This gives us 
the following expression
\[
	\P_{\text{FA}} = \frac{|V|}{(\max_{\post}t(\post)) - |P|}
\]
Figure \ref{fig:norm_fa_score} shows an example of how $\P_{\text{FA}}$ is 
calculated.

\begin{figure}
\begin{center}
	\input{diagrams/fa_diag}
\end{center}
\caption{An example of calculating the maximum number of visits given a thread.  
}
\label{fig:norm_fa_score}
\end{figure}


With these two normalised forms of the original metrics, we can use a weighted 
mean to give a weighted combined form of the two error rates, 
$\P_{\text{error}}$:
\[
	\prerror = \alpha\P_{\text{FA}} + (1-\alpha)\P_{\text{miss}}
\]


In the following chapters, we will discuss the results of our experiments with 
the various algorithms found in the previous chapter, and measure their 
effectiveness using the following metrics:
\begin{itemize}
	\item \textbf{$T$-scores}: the average time taken for a post to be retrieved 
		after it is posted (in minutes)
	\item \textbf{Visit/Post ratio}: the average number of visits required 
		before a post is retrieved.
	\item $\prerror$: a combined, normalised score representing the above two 
		metrics.
\end{itemize}

