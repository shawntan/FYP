With the increasing number of sites leveraging user-generated content, a method 
for predicting the updates of such sites needs to be created in order for an 
incremental web crawler to effectively crawl the site. Our high level goal: to 
predict the posting behaviour of users to such sites.

While this primary goal has many challenges, in this report, we have chosen to 
address challenges specific to forum threads. We want to predict, given content 
of the current thread, the time at which a user would post to the thread. We 
evaluate three different machine learning approaches: Two offline algorithms, 
one that only takes into account only the latest window, and another that 
accounts for past windows, with decreasing weightage. And an online algorithm, 
that uses gradient descent to update its weights every time a new post is 
observed.

Overall, our evaluation shows that our methods work better than the baseline, 
which was to revisit the thread at the average time interval. These are 
promising results, and more can be done to improve upon them. 

There are, however, limitations with the current methods, such as....


\section{Contributions}
We have made the following contributions with our work:
\begin{enumerate}
\item Provide comparable evaluation metrics that can be parameterised, depending 
on the evaluators' priority: freshness or bandwidth.

\item Three different prediction methods using machine learning.
\item The effectiveness of using content and time differences for post 
prediction.
\end{enumerate}

\section{Future Work}

With the proposed methods still having many shortcomings when predicting new 
posts, or providing insight into what causes post arrival times, it leaves much 
room for future work to be done.

\subsection{Using Natural Language Processing (NLP) techniques}
There has been some work done recently in predicting events in social media, and 
in particular, tweets.  \outcite{Wang} dealt with predicting the retweetability 
of tweets using content. They applied two levels of classification, the first 
level categorising tweets into 6 different types: Opinion, Update, Interaction, 
Fact, Deals and Others. This was done using similar techniques as 
\outcite{Sriram2010} and \outcite{Naaman2010}. The Opinion and Update categories 
are then further categorised into another three and two sub-categories each. The 
authors performed this categorisation using labeled Latent Dirichlet Allocation.

The task in this work, was to predict which of three predefined classes a tweet 
will fall into: no retweets, a low number of retweets and a high number of retweets. 

\subsection{Topic modelling}
\cite{Gonzalez2005,Hsu2006}

\subsection{Leveraging context}


\subsection{Using online learning techniques}
Since the purpose of our envisioned model would be to create a crawler that can 
estimate the best times to revisit a page, a proper approach to modeling this as 
a time-series model would be through machine learning techniques.

An interesting approach to forecasting stock prices was presented in 
\outcite{Cao2003}. The technique involved tweaking conventional SVMs to weigh 
recent training instances more heavily than older instances. This is a 
particularly useful idea, since we face the same issue in our task: Recent posts 
are more descriptive of the current state of the thread, and hence should be 
more useful in predicting the next post.
